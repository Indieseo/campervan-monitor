# üéØ **CLAUDE CODE ULTIMATE SCRAPING PROMPT**

## **MISSION:** Complete 365-Day Vehicle Data Extraction with 100% Success Rate

### **CRITICAL REQUIREMENTS:**
- ‚úÖ **ALL 8 companies** must have live data
- ‚úÖ **ALL vehicle models** for each company
- ‚úÖ **365 days** of pricing data (next full year)
- ‚úÖ **Self-checking and validation** system
- ‚úÖ **Automatic retry** with failsafe mechanisms
- ‚úÖ **Progress tracking** and completion verification
- ‚úÖ **NO STOPPING** until 100% complete

---

## üö® **CURRENT STATUS:**
- **Working:** 5/8 companies (Roadsurfer, Camperdays, Goboony, Outdoorsy, RVshare)
- **Failing:** 3/8 companies (McRent, Yescapa, Cruise America)
- **Target:** 8/8 companies with 365-day data

---

## üõ†Ô∏è **ULTIMATE SCRAPING ARCHITECTURE:**

### **1. Master Controller System**
```python
class UltimateScrapingController:
    def __init__(self):
        self.target_companies = [
            'Roadsurfer', 'Camperdays', 'Goboony', 'Outdoorsy', 'RVshare',
            'McRent', 'Yescapa', 'Cruise America'
        ]
        self.target_days = 365
        self.max_retries = 10
        self.completion_threshold = 100.0  # 100% required
        
    def run_until_complete(self):
        """Main loop - runs until 100% completion"""
        while True:
            status = self.check_completion_status()
            if status['completion_percentage'] >= self.completion_threshold:
                self.final_validation()
                break
            else:
                self.execute_scraping_cycle()
                self.validate_results()
                self.retry_failures()
```

### **2. Self-Checking and Validation System**
```python
class ValidationEngine:
    def validate_company_data(self, company, data):
        """Validate data completeness for a company"""
        checks = {
            'has_vehicles': len(data.get('vehicles', [])) > 0,
            'has_365_days': len(data.get('daily_prices', [])) >= 365,
            'has_valid_prices': all(price > 0 for price in data.get('prices', [])),
            'has_currency': data.get('currency') in ['EUR', 'USD'],
            'has_timestamps': all('timestamp' in day for day in data.get('daily_prices', []))
        }
        return {
            'valid': all(checks.values()),
            'checks': checks,
            'completion_percentage': sum(checks.values()) / len(checks) * 100
        }
    
    def validate_all_data(self, all_data):
        """Validate all companies' data"""
        results = {}
        for company, data in all_data.items():
            results[company] = self.validate_company_data(company, data)
        return results
```

### **3. Advanced Retry and Failsafe System**
```python
class FailsafeRetrySystem:
    def __init__(self):
        self.retry_strategies = [
            'different_browser',
            'different_user_agent',
            'proxy_rotation',
            'mobile_user_agent',
            'incognito_mode',
            'javascript_disabled',
            'different_timeout',
            'manual_intervention'
        ]
        self.max_retries_per_strategy = 3
        
    def retry_with_strategy(self, company, strategy):
        """Retry scraping with specific strategy"""
        if strategy == 'different_browser':
            return self.scrape_with_firefox(company)
        elif strategy == 'proxy_rotation':
            return self.scrape_with_proxy(company)
        elif strategy == 'mobile_user_agent':
            return self.scrape_mobile(company)
        # ... implement all strategies
        
    def escalate_failure(self, company, all_strategies_failed):
        """Escalate to manual intervention if all strategies fail"""
        self.create_manual_intervention_ticket(company)
        self.notify_admin(company, all_strategies_failed)
```

---

## üéØ **SPECIFIC IMPLEMENTATION TASKS:**

### **Task 1: Create Master Scraping Controller**
```python
# File: scrapers/master_scraping_controller.py
class MasterScrapingController:
    def __init__(self):
        self.companies = self.load_company_configs()
        self.target_days = 365
        self.completion_threshold = 100.0
        self.retry_limit = 10
        
    def run_until_complete(self):
        """Main execution loop - NEVER STOPS until 100% complete"""
        attempt = 0
        while True:
            attempt += 1
            logger.info(f"üîÑ SCRAPING CYCLE {attempt}")
            
            # Check current status
            status = self.get_completion_status()
            logger.info(f"üìä Current completion: {status['overall_percentage']:.1f}%")
            
            if status['overall_percentage'] >= self.completion_threshold:
                logger.info("üéâ 100% COMPLETION ACHIEVED!")
                self.final_validation()
                break
            
            # Execute scraping for incomplete companies
            self.scrape_incomplete_companies()
            
            # Validate results
            validation_results = self.validate_all_results()
            
            # Retry failures
            self.retry_failed_companies(validation_results)
            
            # Wait before next cycle
            time.sleep(60)  # 1 minute between cycles
```

### **Task 2: Implement 365-Day Data Extraction**
```python
# File: scrapers/calendar_data_extractor.py
class CalendarDataExtractor:
    def extract_365_days(self, company, start_date=None):
        """Extract pricing data for next 365 days"""
        if start_date is None:
            start_date = datetime.now()
        
        all_daily_data = []
        
        # Extract data in chunks to avoid timeouts
        chunk_size = 30  # 30 days per chunk
        for chunk_start in range(0, 365, chunk_size):
            chunk_end = min(chunk_start + chunk_size, 365)
            
            chunk_data = self.extract_date_range(
                company, 
                start_date + timedelta(days=chunk_start),
                start_date + timedelta(days=chunk_end)
            )
            
            all_daily_data.extend(chunk_data)
            
            # Validate chunk before proceeding
            if not self.validate_chunk(chunk_data):
                raise Exception(f"Chunk validation failed for {company}")
        
        return all_daily_data
    
    def extract_date_range(self, company, start_date, end_date):
        """Extract data for specific date range"""
        # Implementation for each company's specific scraping logic
        pass
```

### **Task 3: Create Self-Validation System**
```python
# File: scrapers/self_validation_system.py
class SelfValidationSystem:
    def validate_completion(self, all_data):
        """Validate that all requirements are met"""
        validation_results = {
            'companies_complete': 0,
            'total_companies': len(all_data),
            'companies_with_365_days': 0,
            'companies_with_vehicles': 0,
            'overall_completion': 0.0
        }
        
        for company, data in all_data.items():
            company_validation = self.validate_company(company, data)
            validation_results['companies_complete'] += company_validation['complete']
            validation_results['companies_with_365_days'] += company_validation['has_365_days']
            validation_results['companies_with_vehicles'] += company_validation['has_vehicles']
        
        validation_results['overall_completion'] = (
            validation_results['companies_complete'] / 
            validation_results['total_companies'] * 100
        )
        
        return validation_results
    
    def validate_company(self, company, data):
        """Validate individual company data"""
        return {
            'complete': self.check_365_days(data) and self.check_vehicles(data),
            'has_365_days': len(data.get('daily_prices', [])) >= 365,
            'has_vehicles': len(data.get('vehicles', [])) > 0,
            'data_quality': self.check_data_quality(data)
        }
```

### **Task 4: Implement Failsafe Mechanisms**
```python
# File: scrapers/failsafe_system.py
class FailsafeSystem:
    def __init__(self):
        self.failure_count = {}
        self.max_failures = 10
        self.escalation_threshold = 5
        
    def handle_failure(self, company, error):
        """Handle scraping failures with escalation"""
        if company not in self.failure_count:
            self.failure_count[company] = 0
        
        self.failure_count[company] += 1
        
        if self.failure_count[company] >= self.escalation_threshold:
            self.escalate_to_manual(company, error)
        else:
            self.retry_with_different_strategy(company)
    
    def escalate_to_manual(self, company, error):
        """Escalate to manual intervention"""
        logger.error(f"üö® ESCALATING {company} TO MANUAL INTERVENTION")
        logger.error(f"Error: {error}")
        
        # Create manual intervention file
        intervention_data = {
            'company': company,
            'error': str(error),
            'timestamp': datetime.now().isoformat(),
            'failure_count': self.failure_count[company],
            'required_action': 'Manual scraping required'
        }
        
        with open(f"output/manual_intervention_{company}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
            json.dump(intervention_data, f, indent=2)
```

---

## üîÑ **LOOPING AND PERSISTENCE FEATURES:**

### **1. Persistent State Management**
```python
class PersistentStateManager:
    def __init__(self):
        self.state_file = "output/scraping_state.json"
        self.load_state()
    
    def save_progress(self, company, progress_data):
        """Save progress for specific company"""
        self.state[company] = {
            'last_updated': datetime.now().isoformat(),
            'progress': progress_data,
            'completion_percentage': self.calculate_completion(progress_data)
        }
        self.save_state()
    
    def load_state(self):
        """Load previous state if exists"""
        if Path(self.state_file).exists():
            with open(self.state_file, 'r') as f:
                self.state = json.load(f)
        else:
            self.state = {}
    
    def get_incomplete_companies(self):
        """Get list of companies that need more work"""
        incomplete = []
        for company, data in self.state.items():
            if data.get('completion_percentage', 0) < 100:
                incomplete.append(company)
        return incomplete
```

### **2. Automatic Retry Logic**
```python
class AutoRetrySystem:
    def __init__(self):
        self.retry_schedules = {
            'immediate': 0,      # Retry immediately
            'short': 300,        # 5 minutes
            'medium': 1800,      # 30 minutes
            'long': 3600,        # 1 hour
            'daily': 86400       # 24 hours
        }
    
    def schedule_retry(self, company, retry_type='medium'):
        """Schedule retry for failed company"""
        delay = self.retry_schedules[retry_type]
        retry_time = datetime.now() + timedelta(seconds=delay)
        
        retry_data = {
            'company': company,
            'retry_time': retry_time.isoformat(),
            'retry_type': retry_type,
            'attempt_count': self.get_attempt_count(company) + 1
        }
        
        self.save_retry_schedule(retry_data)
    
    def check_retry_schedule(self):
        """Check if any companies are ready for retry"""
        ready_for_retry = []
        current_time = datetime.now()
        
        for retry_data in self.load_retry_schedules():
            retry_time = datetime.fromisoformat(retry_data['retry_time'])
            if current_time >= retry_time:
                ready_for_retry.append(retry_data['company'])
        
        return ready_for_retry
```

---

## üìä **PROGRESS TRACKING AND REPORTING:**

### **1. Real-Time Progress Dashboard**
```python
class ProgressTracker:
    def __init__(self):
        self.progress_file = "output/live_progress.json"
        
    def update_progress(self, company, status):
        """Update progress for specific company"""
        progress_data = {
            'timestamp': datetime.now().isoformat(),
            'company': company,
            'status': status,
            'completion_percentage': self.calculate_completion(status),
            'vehicles_found': len(status.get('vehicles', [])),
            'days_scraped': len(status.get('daily_prices', [])),
            'target_days': 365
        }
        
        self.save_progress(progress_data)
        self.generate_progress_report()
    
    def generate_progress_report(self):
        """Generate comprehensive progress report"""
        report = {
            'overall_completion': self.calculate_overall_completion(),
            'companies_status': self.get_all_companies_status(),
            'estimated_completion_time': self.estimate_completion_time(),
            'next_actions': self.get_next_actions()
        }
        
        with open("output/progress_report.json", 'w') as f:
            json.dump(report, f, indent=2)
```

### **2. Completion Verification System**
```python
class CompletionVerifier:
    def verify_100_percent_completion(self, all_data):
        """Verify that 100% completion has been achieved"""
        verification_results = {
            'all_companies_complete': True,
            'all_365_days_present': True,
            'all_vehicles_found': True,
            'data_quality_valid': True,
            'overall_verification': True
        }
        
        for company, data in all_data.items():
            company_verification = self.verify_company_completion(company, data)
            
            verification_results['all_companies_complete'] &= company_verification['complete']
            verification_results['all_365_days_present'] &= company_verification['has_365_days']
            verification_results['all_vehicles_found'] &= company_verification['has_vehicles']
            verification_results['data_quality_valid'] &= company_verification['quality_valid']
        
        verification_results['overall_verification'] = all(verification_results.values())
        
        return verification_results
```

---

## üöÄ **EXECUTION COMMANDS:**

### **1. Start Ultimate Scraping**
```bash
# Start the master controller
python scrapers/master_scraping_controller.py

# Monitor progress in real-time
python scrapers/progress_monitor.py

# Check completion status
python scrapers/completion_checker.py
```

### **2. Individual Company Testing**
```bash
# Test specific failing companies
python scrapers/test_mcrent_365_days.py
python scrapers/test_yescapa_365_days.py
python scrapers/test_cruise_america_365_days.py
```

### **3. Validation and Verification**
```bash
# Validate all data
python scrapers/validate_all_data.py

# Generate completion report
python scrapers/generate_completion_report.py

# Check for missing data
python scrapers/find_missing_data.py
```

---

## üéØ **SUCCESS CRITERIA:**

### **100% Completion Requirements:**
1. **All 8 companies** have data
2. **All vehicle models** identified and scraped
3. **365 days** of pricing data for each company
4. **Data quality** validation passed
5. **No missing data** in any category
6. **Real-time verification** completed

### **Final Validation Checklist:**
- [ ] Roadsurfer: 365 days + all vehicles
- [ ] Camperdays: 365 days + all vehicles  
- [ ] Goboony: 365 days + all vehicles
- [ ] Outdoorsy: 365 days + all vehicles
- [ ] RVshare: 365 days + all vehicles
- [ ] McRent: 365 days + all vehicles
- [ ] Yescapa: 365 days + all vehicles
- [ ] Cruise America: 365 days + all vehicles

---

## üõ°Ô∏è **FAILSAFE FEATURES:**

### **1. Automatic Recovery**
- **Browser crashes:** Auto-restart with state recovery
- **Network failures:** Retry with exponential backoff
- **Rate limiting:** Automatic delay and retry
- **Memory issues:** Cleanup and restart

### **2. Manual Intervention Triggers**
- **10+ consecutive failures** for same company
- **Data quality below threshold**
- **Missing critical data** after 24 hours
- **System resource exhaustion**

### **3. Progress Persistence**
- **State saved every 5 minutes**
- **Resume from last checkpoint**
- **No data loss on restart**
- **Complete audit trail**

---

## üéØ **FINAL GOAL:**

**ACHIEVE 100% COMPLETION WITH:**
- ‚úÖ All 8 companies working
- ‚úÖ All vehicle models identified
- ‚úÖ 365 days of pricing data
- ‚úÖ Self-validated results
- ‚úÖ Zero missing data
- ‚úÖ Production-ready dataset

**THE SYSTEM WILL NOT STOP UNTIL 100% COMPLETION IS ACHIEVED!** üöÄ

---

## üìù **DELIVERABLES:**

1. **Complete dataset** with 365 days for all companies
2. **Validation report** confirming 100% completion
3. **Progress logs** showing all attempts and results
4. **Screenshot evidence** for all successful scrapes
5. **Manual intervention files** for any unresolved issues
6. **Final dashboard** showing all data in calendar format

**THIS IS THE ULTIMATE SCRAPING CHALLENGE - NO EXCUSES, NO STOPPING, 100% SUCCESS REQUIRED!** üí™



